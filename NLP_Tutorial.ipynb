{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial and Project:\n",
    "### NLP Data Preparation: \n",
    "### Clean of the Data:\n",
    "download the txt file from http://www.gutenberg.org/cache/epub/5200/pg5200.txt and remove the header and footer and renamed as metamorphosis_clean.txt file.\n",
    "\n",
    "#### We will see by only :\n",
    "i) Cleaning Manually and with NLTK &\n",
    "\n",
    "ii) Prepare text data with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin']\n"
     ]
    }
   ],
   "source": [
    "#i)Clean Manually and with NLTK\n",
    "# Load the Data\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into words by white space \"\"(space),new lines,tabs and more\n",
    "words=text.split()\n",
    "print(words[:70]) # display first 70 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# other way\n",
    "# Load the Data\n",
    "import re  # regex model(re)\n",
    "#regex model (re) and split the document into words by selecting for strings of alphanumeric characters \n",
    "#(a-z, A-Z, 0-9 and ‘ ’). \n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into words by white space \"\"(space),new lines,tabs and more\n",
    "words=re.split(r'\\W+',text)\n",
    "print(words[:70]) # display first 70 words . this would \n",
    "#This time, we can see that armour-like is now two words armour and like (ﬁne) but contractions like What’s \n",
    "# is also two words What and s (not great). So this not so helpful .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# print punctation\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "### Remove the Punctuation\n",
    "#prepare regex for char filtering\n",
    "re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "#remove punctuation from each word\n",
    "stripped=[re_punc.sub('',w) for w in words] # use of sub() to replace punctuation character with \n",
    "# nothing.\n",
    "print(stripped[:70])\n",
    "#We can see that this has had the desired eﬀect, mostly. Contractions like What’s have become Whats but \n",
    "#armour-like has become armourlike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# sometimes the text data may contain non-printable character. use similar approach to filter out\n",
    "# all non-printable characters by selecting the inverse of the string.printable constant.\n",
    "#prepare regex for char filtering\n",
    "re_print=re.compile('[^%s]'% re.escape(string.printable))\n",
    "\n",
    "result=[re_print.sub('',w) for w in words] # use of sub() to replace punctuation character with \n",
    "# nothing.\n",
    "print(result[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# NOrmalizing Case\n",
    "# converting all words to lowecase. This means that vocabulary will shrink in size,but\n",
    "# some distinctions are lost(Apple company vs apple fruits)\n",
    "words= [word.lower() for word in words]\n",
    "print(words[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and cleaning with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# command to instal the nltk library(one time)\n",
    "#import nltk\n",
    "#The Natural Language Toolkit, or NLTK for short\n",
    "#nltk.download()\n",
    "#Cleaning text is really hard, problem speciﬁc, and full of tradeoﬀs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "# Split text into sentence:\n",
    "from nltk import sent_tokenize\n",
    "#load the data\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into the sentence\n",
    "sentence=sent_tokenize(text)\n",
    "print(sentence[0])  # print the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide']\n"
     ]
    }
   ],
   "source": [
    "# Split sentence into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "#It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate\n",
    "#tokens. Contractions are split apart (e.g. What’s becomes What and ’s). Quotes are kept, and so on. \n",
    "tokens=word_tokenize(text)\n",
    "print(tokens[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared']\n"
     ]
    }
   ],
   "source": [
    "# Filter out punctuation: filter out standalone punctuation. isalpha() function is used.\n",
    "words=[word for word in tokens if word.isalpha()]\n",
    "#iterating over all tokens and only keeping those tokens that are all alphabetic. Python has the function \n",
    "#isalpha() that can be used\n",
    "print(words[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words ( and pipeline): Stop words do not contribute to the deeper meaning of the\n",
    "# phrase. Like the,a, and, is , etc\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')  # we are only interested in english language.\n",
    "print(stop_words)  # shows full list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pipeline of text preparation\n",
    " Load the raw text.\n",
    "\n",
    " Split into tokens.\n",
    "\n",
    " Convert to lowercase.\n",
    "\n",
    " Remove punctuation from each token.\n",
    "\n",
    " Filter out remaining tokens that are not alphabetic.\n",
    "\n",
    " Filter out tokens that are stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words ( and pipeline): Stop words do not contribute to the deeper meaning of the\n",
    "# phrase. Like the,a, and, is , etc\n",
    "\n",
    "# PIPELINE: PROCESS:\n",
    "    # import the necessary library\n",
    "import string\n",
    "import re   # regex\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "    # load dataset\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "    # split into words\n",
    "tokens=word_tokenize(text)\n",
    "    # convert to lower case\n",
    "tokens= [w.lower() for w in tokens]\n",
    "    # prepare regex for char filtering\n",
    "re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "stripped= [re_punc.sub('',w) for w in tokens]\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "words= [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "words=[ w for w in words if not w in stop_words]\n",
    "print(words[:77])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I note that we are still left with tokens like nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'wave', 'helplessli', 'look', 'happen', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'littl', 'small', 'lay', 'peac', 'four', 'familiar', 'wall', 'collect', 'textil', 'sampl', 'lay', 'spread', 'tabl', 'samsa', 'travel', 'salesman', 'hung', 'pictur', 'recent', 'cut', 'illustr', 'magazin']\n"
     ]
    }
   ],
   "source": [
    "# Stem Words:\n",
    "   # stemming refers to the process of reducing each word to its root or base. For example\n",
    "    # fishing,fished,fisher , all reduces to stem fish. It is used to focus on sense or sentiment\n",
    "    # of a document rather than deeper meaning.There are many stemming algorithms, the popular\n",
    "    # one is Porter Stemming algorithm.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter= PorterStemmer()    # import \n",
    "stemmed=[porter.stem(word) for word in words]\n",
    "print(stemmed[:77])\n",
    "#There is a nice suite of stemming and lemmatization algorithms to choose from in NLTK, if reducing words\n",
    "#to their root is something you need for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition Text cleaning cosiderations:\n",
    "Since are above data is almost clean , we may encounter a raw data which will be much messy.\n",
    "Here is a shortlist of additional considerations when cleaning text:\n",
    "\n",
    " Handling large documents and large collections of text documents that do not ﬁt into memory.\n",
    "\n",
    " Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "\n",
    " Transliteration of characters from other languages into English.\n",
    "\n",
    " Decoding Unicode characters into a normalized form, such as UTF8(Unicode Transformation Format 8 bit).\n",
    "\n",
    " Handling of domain speciﬁc words, phrases, and acronyms.\n",
    "\n",
    " Handling or removing numbers, such as dates and amounts.\n",
    "\n",
    " Locating and correcting common typos and misspellings.\n",
    "\n",
    " And much more...\n",
    "\n",
    "The list could go on. Hopefully, you can see that getting truly clean text is impossible, that we are really doing the best we can based on the time, resources, and knowledge we have. The idea of clean is really deﬁned by the speciﬁc task or concern of your project. A pro tip is to continually review your tokens after every transform. I have tried to show that in this tutorial and I hope you take that to heart. Ideally, you would save a new ﬁle after each transform so that you can spend time with all of the data in the new form. Things always jump out at you when to take the time to review your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text Data with scikit-learn\n",
    "Text data requires special preparation before you can start using it for predictive modeling. The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or ﬂoating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "We will see  Python with scikit-learn doing below:\n",
    "\n",
    " How to convert text to word count vectors with CountVectorizer.\n",
    "\n",
    "i) Create an instance of the CountVectorizer class.\n",
    "\n",
    "ii) Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "\n",
    "iii) Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "\n",
    " How to convert text to word frequency vectors with TfidfVectorizer.\n",
    "\n",
    "i) Term Frequency(Tf): This summarizes how often a given word appears within a document.\n",
    "\n",
    "ii) Inverse Document Frequency(idf): This downscales words that appear a lot across documents\n",
    "\n",
    " How to convert text to unique integers with HashingVectorizer:\n",
    "\n",
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large. This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms. A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long ﬁxed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Prepare text data with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good ﬁrst step when working with text is to split it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the text to word sequence() function that you can use to split text into a list of words. By default, this function automatically does 3 things:\n",
    "\n",
    " Splits words by space.\n",
    "\n",
    " Filters out punctuation.\n",
    "\n",
    " Converts text to lowercase (lower=True).\n",
    "\n",
    "You can change any of these defaults by passing arguments to the function. Below is an example of using the text to word sequence() function to split a document (in this case a simple string) into a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# Let take the text\n",
    "text='The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "words=text_to_word_sequence(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding with one hot\n",
    "It is popular to represent a document as a sequence of integer values, where each word in the document is represented as a unique integer. Keras provides the one hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one hot encoding of the document, which is not the case. Instead, the function is a wrapper for the hashing trick() function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values. As with the text to word sequence() function in the previous section, the one hot() function will make the text lower case, ﬁlter out punctuation, and split words based on white space.\n",
    "\n",
    "In addition to the text, the vocabulary size (total words) must be speciﬁed. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary deﬁnes the hashing space from which words are hashed. By default, the hash function is used, although as we will see in the next section, alternate hash functions can be speciﬁed when calling the hashing trick() function directly.\n",
    "\n",
    "We can use the text to word sequence() function from the previous section to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1, 6, 4, 3, 2, 8, 1, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "# estimate the size of the vocabulary\n",
    "words= set(text_to_word_sequence(text))\n",
    "vocab_size=len(words)\n",
    "print(vocab_size)\n",
    "# Integer encode the document.\n",
    "result= one_hot(text,round(vocab_size*1.3)) # the vocabulary size is increased by one-third to\n",
    "# minimize collision when hashing words.\n",
    "print(result) # gives the array of integer encoded words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Encoding with hashing trick\n",
    "A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers. An alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n",
    "\n",
    "Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function. It provides more ﬂexibility, allowing you to specify the hash function as either hash (the default) or other hash functions such as the built in md5 function or your own function. Below is an example of integer encoding a document using the md5 hash function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "# Let take the text\n",
    "text='The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "words=text_to_word_sequence(text)\n",
    "# estimate the size of the vocabulary\n",
    "words= set(text_to_word_sequence(text))\n",
    "vocab_size=len(words)\n",
    "print(vocab_size)\n",
    "# Integer encode the document.\n",
    "result= hashing_trick(text,round(vocab_size*1.3),hash_function='md5') # the vocabulary size is \n",
    "# increased by one-third to minimize collision when hashing words.\n",
    "print(result) # gives the array of integer encoded words\n",
    "#keras.preprocessing.text.hashing_trick(text, n, hash_function=None,\n",
    "#filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') where\n",
    "#n: Dimension of the hashing space. and Note that 'hash' is not a stable hashing function,\n",
    "#so it is not consistent across different runs, while 'md5' is a stable hashing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer API\n",
    "So far we have looked at one-oﬀ convenience methods for preparing text with Keras. Keras provides a more sophisticated API for preparing text that can be ﬁt and reused to prepare multiple text documents. This may be the preferred approach for large projects. Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then ﬁt on either raw text documents or integer encoded text documents.\n",
    "\n",
    "Once ﬁt, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
    "\n",
    " word counts: A dictionary mapping of words and their occurrence counts when the Tokenizer was ﬁt.\n",
    "\n",
    " word docs: A dictionary mapping of words and the number of documents that reach appears in.\n",
    "\n",
    " word index: A dictionary of words and their uniquely assigned integers.\n",
    "\n",
    "\n",
    " document count: A dictionary mapping and the number of documents they appear in calculated during the ﬁt.\n",
    "\n",
    "Once the Tokenizer has been ﬁt on training data, it can be used to encode documents in the train or test datasets. The texts_to_matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:\n",
    "\n",
    " binary: Whether or not each word is present in the document. This is the default.\n",
    "\n",
    " count: The count of each word in the document.\n",
    "\n",
    " tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "\n",
    "i) Term Frequency(Tf): This summarizes how often a given word appears within a document.\n",
    "\n",
    "ii) Inverse Document Frequency(idf): This downscales words that appear a lot across documents\n",
    "\n",
    " freq: The frequency of each word as a ratio of words within each document.\n",
    "\n",
    "We can put all of this together with a worked example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "\n",
      "\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
      "\n",
      "\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "# define 5 documents \n",
    "docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs) # which gives 4 attributes, printing these attributes as follows\n",
    "# Printing out the 4 attributes of fit_on_texts function\n",
    "print(t.word_counts)\n",
    "print('\\n')\n",
    "print(t.document_count)\n",
    "print('\\n')\n",
    "print(t.word_index) \n",
    "print('\\n')\n",
    "print(t.word_docs)\n",
    "print('\\n')\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count') #create one vector per document provided per input\n",
    "print(encoded_docs)\n",
    "#Running the example ﬁts the Tokenizer with 5 small documents. The details of the ﬁt Tokenizer are printed.\n",
    "#Then the 5 documents are encoded using a word count. Each document is encoded as a 9-element vector with one \n",
    "#position for each word and the chosen encoding scheme value for each word position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bag-of-Words Model\n",
    "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classiﬁcation. \n",
    " Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Speciﬁcally, vectors of numbers.\n",
    " In language processing, the vectors x are derived from textual data, in order to reﬂect various linguistic properties of the text. Mapping from textual data to real valued vectors is called feature extraction or feature representation,and is done by a feature function.Deciding on the right features is an integral part of a successful machine learning project.\n",
    " A `bag-of-words model`, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    " A bag-of-words is a representation of text that describes the occurrence of words within a document. or in simple language\n",
    " A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature. For e.g.\n",
    " It was the best of times,\n",
    " \n",
    " it was the worst of times, \n",
    " \n",
    " it was the age of wisdom, \n",
    " \n",
    " it was the age of foolishness,\n",
    " \n",
    "let’s treat each line as a separate document and the 4 lines as our entire corpus of documents.\n",
    "  There are 10 unique words out of 24 corpus words. The next step is to score the words in each document. The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present. Using the arbitrary ordering of words listed above in our vocabulary, we can step through the ﬁrst document (It was the best of times) and convert it into a binary vector. The scoring of the document would look as follows:\n",
    "  \n",
    "it = 1 was = 1 the = 1 best = 1 of = 1 times = 1 worst = 0 age = 0 wisdom = 0 foolishness = 0 \n",
    "\n",
    "i.e. [1, 1, 1, 1, 1, 1, 0, 0, 0, 0] ( 10 unique words consideration )\n",
    "\n",
    "similaryly, for second documents: \"it was the worst of times\" \n",
    "\n",
    "it = 1 was = 1 the = 1 best = 0 of = 1 times = 1 worst = 1 age = 0 wisdom = 0 foolishness = 0\n",
    "\n",
    "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0] and similary doing for others.\n",
    "\n",
    "As the vocabulary size increases, so does the vector representation of documents.imagine that for a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary. This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document. In this approach, each word or token is called a gram. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams.An n-gram is an n-token sequence of words, where n refers to the number of grouped words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “please turn”, “turn your”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”.\n",
    "#### Word Hashing\n",
    "You may remember from computer science that a hash function is a bit of math that `maps data to a ﬁxed size set of numbers`. For example, we use them in hash tables when programming where perhaps names are converted to numbers for fast lookup. We can use a hash representation of known words in our vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, which is in turn the size of the vector representation of the document. Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. This is called the hash trick or feature hashing. The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-oﬀ sparsity.\n",
    "##### Limitations of Bag-of-Words\n",
    "The bag-of-words model is very simple to understand and implement and oﬀers a lot of ﬂexibility for customization on your speciﬁc text data. It has been used with great success on prediction problems like language modeling and documentation classiﬁcation. Nevertheless, it suﬀers from some shortcomings, such as:\n",
    "\n",
    " Vocabulary: The vocabulary requires careful design, most speciﬁcally in order to manage the size, which impacts the sparsity of the document representations.\n",
    "\n",
    " Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "\n",
    " Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can oﬀer a lot to the model, that if modeled could tell the diﬀerence between the same words diﬀerently arranged (this is interesting vs is this interesting), synonyms (old bike vs used bike), and much more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
